{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a83ef2",
   "metadata": {},
   "source": [
    "# ⚽ FIFA Analytics Dashboard - Complete Setup & Usage Guide\n",
    "\n",
    "## 🚀 Professional Sports Analytics with Python\n",
    "\n",
    "Welcome to the comprehensive guide for the **FIFA Analytics Dashboard** - a modern, machine learning-powered analytics platform for football data analysis. This notebook provides step-by-step instructions, commands, and best practices for setting up and using the dashboard effectively.\n",
    "\n",
    "### 📋 What You'll Learn\n",
    "\n",
    "- **Environment Setup**: Create and manage Python virtual environments\n",
    "- **Dependency Management**: Install and troubleshoot required packages\n",
    "- **Data Validation**: Ensure your FIFA data is properly structured\n",
    "- **Application Commands**: Run multiple Streamlit dashboards simultaneously\n",
    "- **Performance Optimization**: Handle large datasets efficiently\n",
    "- **Troubleshooting**: Solve common setup and runtime issues\n",
    "- **Deployment**: Prepare for production environments\n",
    "\n",
    "### 🎯 Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- Basic command line knowledge\n",
    "- FIFA player dataset (CSV format)\n",
    "- 4GB+ RAM (for large datasets)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's build a world-class football analytics platform together!** 🏆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f37363",
   "metadata": {},
   "source": [
    "# 🔧 Section 1: Environment Setup and Virtual Environment Creation\n",
    "\n",
    "## Understanding Virtual Environments\n",
    "\n",
    "Virtual environments are isolated Python environments that allow you to manage dependencies for different projects separately. This prevents conflicts between package versions and ensures reproducible deployments.\n",
    "\n",
    "### Why Use Virtual Environments?\n",
    "- **Isolation**: Keep project dependencies separate\n",
    "- **Reproducibility**: Ensure consistent environments across different machines\n",
    "- **Version Control**: Manage different package versions for different projects\n",
    "- **Clean System**: Avoid cluttering your global Python installation\n",
    "\n",
    "## Creating Virtual Environments\n",
    "\n",
    "### Method 1: Using Python venv (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows Environment Setup Commands\n",
    "# Run these in Command Prompt or PowerShell\n",
    "\n",
    "# 1. Navigate to your project directory\n",
    "# cd \"D:\\Big Data\\Kişisel öğrenim\\fifa_dash\"\n",
    "\n",
    "# 2. Create virtual environment\n",
    "# python -m venv venv\n",
    "\n",
    "# 3. Activate virtual environment (Windows Command Prompt)\n",
    "# venv\\Scripts\\activate\n",
    "\n",
    "# 4. Activate virtual environment (Windows PowerShell)\n",
    "# venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# 5. Verify Python version and virtual environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check current Python environment details\"\"\"\n",
    "    print(\"🐍 Python Environment Information\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    print(f\"Python Executable: {sys.executable}\")\n",
    "    print(f\"Virtual Environment: {'Yes' if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix) else 'No'}\")\n",
    "    print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "    print(f\"Python Path: {sys.path[0]}\")\n",
    "    \n",
    "    # Check if running in virtual environment\n",
    "    in_venv = (hasattr(sys, 'real_prefix') or \n",
    "               (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix))\n",
    "    \n",
    "    if in_venv:\n",
    "        print(\"✅ Running in virtual environment\")\n",
    "    else:\n",
    "        print(\"⚠️  Running in global Python environment\")\n",
    "    \n",
    "    return in_venv\n",
    "\n",
    "# Execute the check\n",
    "environment_status = check_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# macOS/Linux Environment Setup Commands\n",
    "# Run these in Terminal\n",
    "\n",
    "# 1. Navigate to your project directory\n",
    "# cd ~/fifa_dash\n",
    "\n",
    "# 2. Create virtual environment\n",
    "# python3 -m venv venv\n",
    "\n",
    "# 3. Activate virtual environment\n",
    "# source venv/bin/activate\n",
    "\n",
    "# 4. Alternative: Using conda (if Anaconda/Miniconda is installed)\n",
    "# conda create -n fifa_dash python=3.12\n",
    "# conda activate fifa_dash\n",
    "\n",
    "# Cross-platform environment verification\n",
    "def verify_python_setup():\n",
    "    \"\"\"Verify Python setup is suitable for the FIFA Dashboard\"\"\"\n",
    "    print(\"🔍 Python Setup Verification\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Check Python version\n",
    "    python_version = sys.version_info\n",
    "    min_version = (3, 8)\n",
    "    \n",
    "    print(f\"Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "    \n",
    "    if python_version >= min_version:\n",
    "        print(\"✅ Python version is compatible\")\n",
    "    else:\n",
    "        print(f\"❌ Python version too old. Minimum required: {min_version[0]}.{min_version[1]}\")\n",
    "        return False\n",
    "    \n",
    "    # Check available modules for dashboard\n",
    "    required_modules = ['sys', 'os', 'subprocess']\n",
    "    missing_modules = []\n",
    "    \n",
    "    for module in required_modules:\n",
    "        try:\n",
    "            __import__(module)\n",
    "            print(f\"✅ {module} available\")\n",
    "        except ImportError:\n",
    "            missing_modules.append(module)\n",
    "            print(f\"❌ {module} missing\")\n",
    "    \n",
    "    if not missing_modules:\n",
    "        print(\"🎉 Python setup verified successfully!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"⚠️  Missing modules: {missing_modules}\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "setup_ok = verify_python_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f7ada",
   "metadata": {},
   "source": [
    "# 📦 Section 2: Installing Required Dependencies\n",
    "\n",
    "## Understanding the Requirements\n",
    "\n",
    "The FIFA Analytics Dashboard requires several Python packages for data processing, visualization, and machine learning. Here's a breakdown of the key dependencies:\n",
    "\n",
    "### Core Libraries\n",
    "- **streamlit**: Web application framework for the dashboard\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **numpy**: Numerical computing foundation\n",
    "- **plotly**: Interactive visualizations\n",
    "- **scikit-learn**: Machine learning algorithms\n",
    "\n",
    "### Visualization & Analysis\n",
    "- **seaborn**: Statistical data visualization\n",
    "- **matplotlib**: Plotting library\n",
    "- **scipy**: Scientific computing\n",
    "- **statsmodels**: Statistical modeling\n",
    "\n",
    "### Additional Tools\n",
    "- **openpyxl**: Excel file support\n",
    "- **requests**: HTTP library for API calls\n",
    "\n",
    "## Installation Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a075338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation Commands (run in terminal)\n",
    "# pip install -r requirements.txt\n",
    "# OR install packages individually:\n",
    "# pip install streamlit==1.28.1 pandas==2.1.3 numpy==1.25.2 plotly==5.17.0\n",
    "\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from typing import List, Dict\n",
    "\n",
    "def check_package_installation(packages: List[str]) -> Dict[str, bool]:\n",
    "    \"\"\"Check if required packages are installed\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"📋 Checking Package Installation Status\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            # Try to get package information\n",
    "            dist = pkg_resources.get_distribution(package)\n",
    "            results[package] = True\n",
    "            print(f\"✅ {package}: v{dist.version}\")\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            results[package] = False\n",
    "            print(f\"❌ {package}: Not installed\")\n",
    "        except Exception as e:\n",
    "            results[package] = False\n",
    "            print(f\"⚠️  {package}: Error checking - {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def install_missing_packages(missing_packages: List[str]):\n",
    "    \"\"\"Install missing packages using pip\"\"\"\n",
    "    if not missing_packages:\n",
    "        print(\"🎉 All packages are already installed!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🔧 Installing missing packages: {missing_packages}\")\n",
    "    \n",
    "    for package in missing_packages:\n",
    "        try:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"✅ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "# Define required packages for FIFA Dashboard\n",
    "required_packages = [\n",
    "    \"streamlit\",\n",
    "    \"pandas\", \n",
    "    \"numpy\",\n",
    "    \"plotly\",\n",
    "    \"scikit-learn\",\n",
    "    \"seaborn\",\n",
    "    \"matplotlib\",\n",
    "    \"scipy\",\n",
    "    \"statsmodels\"\n",
    "]\n",
    "\n",
    "# Check installation status\n",
    "installation_status = check_package_installation(required_packages)\n",
    "\n",
    "# Find missing packages\n",
    "missing_packages = [pkg for pkg, installed in installation_status.items() if not installed]\n",
    "\n",
    "print(f\"\\n📊 Installation Summary:\")\n",
    "print(f\"   Total packages: {len(required_packages)}\")\n",
    "print(f\"   Installed: {len(required_packages) - len(missing_packages)}\")\n",
    "print(f\"   Missing: {len(missing_packages)}\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n⚠️  Missing packages: {missing_packages}\")\n",
    "    print(\"Run the following command to install them:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n🎉 All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfcc1c",
   "metadata": {},
   "source": [
    "# 📊 Section 3: Data Structure Validation and Loading\n",
    "\n",
    "## Understanding FIFA Data Structure\n",
    "\n",
    "The FIFA Analytics Dashboard expects specific data formats and columns. Proper data validation ensures smooth operation and prevents runtime errors.\n",
    "\n",
    "### Required CSV Files\n",
    "- `male_players.csv` - Male player statistics\n",
    "- `female_players.csv` - Female player statistics\n",
    "\n",
    "### Essential Columns\n",
    "- **player_id**: Unique identifier (integer)\n",
    "- **long_name**: Full player name (string)\n",
    "- **overall**: Overall rating 0-100 (integer)\n",
    "- **potential**: Potential rating 0-100 (integer)\n",
    "- **value_eur**: Market value in Euros (float)\n",
    "- **age**: Player age (integer)\n",
    "- **pace, shooting, passing, dribbling, defending, physic**: Skill ratings (integer)\n",
    "\n",
    "### Optional but Recommended\n",
    "- **club_name**: Current club (string)\n",
    "- **player_positions**: Playing positions (string)\n",
    "- **nationality_name**: Player nationality (string)\n",
    "- **league_name**: Current league (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "def validate_data_structure(file_path: str) -> Dict:\n",
    "    \"\"\"Validate FIFA data file structure and quality\"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'file_exists': False,\n",
    "        'file_readable': False,\n",
    "        'required_columns_present': False,\n",
    "        'data_quality': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    print(f\"🔍 Validating: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"❌ File not found: {file_path}\")\n",
    "        validation_results['recommendations'].append(f\"Create or move {file_path} to the correct location\")\n",
    "        return validation_results\n",
    "    \n",
    "    validation_results['file_exists'] = True\n",
    "    print(f\"✅ File exists: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Try to read the file\n",
    "        df = pd.read_csv(file_path, low_memory=False, nrows=1000)  # Read first 1000 rows for validation\n",
    "        validation_results['file_readable'] = True\n",
    "        print(f\"✅ File readable: {len(df)} rows sampled\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_columns = [\n",
    "            'player_id', 'long_name', 'overall', 'potential', \n",
    "            'value_eur', 'age', 'pace', 'shooting', 'passing', \n",
    "            'dribbling', 'defending', 'physic'\n",
    "        ]\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if not missing_columns:\n",
    "            validation_results['required_columns_present'] = True\n",
    "            print(\"✅ All required columns present\")\n",
    "        else:\n",
    "            print(f\"❌ Missing columns: {missing_columns}\")\n",
    "            validation_results['recommendations'].append(f\"Add missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Data quality checks\n",
    "        quality_checks = {}\n",
    "        \n",
    "        # Check for null values in critical columns\n",
    "        for col in ['player_id', 'long_name', 'overall']:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_percentage = (null_count / len(df)) * 100\n",
    "                quality_checks[f'{col}_null_percentage'] = null_percentage\n",
    "                \n",
    "                if null_percentage == 0:\n",
    "                    print(f\"✅ {col}: No null values\")\n",
    "                elif null_percentage < 5:\n",
    "                    print(f\"⚠️  {col}: {null_percentage:.1f}% null values (acceptable)\")\n",
    "                else:\n",
    "                    print(f\"❌ {col}: {null_percentage:.1f}% null values (problematic)\")\n",
    "                    validation_results['recommendations'].append(f\"Clean null values in {col}\")\n",
    "        \n",
    "        # Check data types\n",
    "        if 'overall' in df.columns:\n",
    "            if df['overall'].dtype in ['int64', 'float64']:\n",
    "                overall_range = (df['overall'].min(), df['overall'].max())\n",
    "                quality_checks['overall_range'] = overall_range\n",
    "                \n",
    "                if 0 <= overall_range[0] and overall_range[1] <= 100:\n",
    "                    print(f\"✅ Overall ratings in valid range: {overall_range}\")\n",
    "                else:\n",
    "                    print(f\"⚠️  Overall ratings outside expected range: {overall_range}\")\n",
    "                    validation_results['recommendations'].append(\"Check overall rating values\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if 'player_id' in df.columns:\n",
    "            duplicate_count = df['player_id'].duplicated().sum()\n",
    "            quality_checks['duplicate_players'] = duplicate_count\n",
    "            \n",
    "            if duplicate_count == 0:\n",
    "                print(\"✅ No duplicate player IDs\")\n",
    "            else:\n",
    "                print(f\"⚠️  {duplicate_count} duplicate player IDs found\")\n",
    "                validation_results['recommendations'].append(\"Remove duplicate player records\")\n",
    "        \n",
    "        validation_results['data_quality'] = quality_checks\n",
    "        \n",
    "        # File size info\n",
    "        file_size_mb = Path(file_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"📁 File size: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "        if file_size_mb > 100:\n",
    "            validation_results['recommendations'].append(\"Consider data sampling for better performance\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading file: {str(e)}\")\n",
    "        validation_results['recommendations'].append(f\"Fix file format or encoding issues: {str(e)}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def load_fifa_data(data_path: str = \"data/\") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load and combine FIFA player data\"\"\"\n",
    "    \n",
    "    print(\"🚀 Loading FIFA Player Data\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    male_file = Path(data_path) / \"male_players.csv\"\n",
    "    female_file = Path(data_path) / \"female_players.csv\"\n",
    "    \n",
    "    # Validate files\n",
    "    male_validation = validate_data_structure(str(male_file))\n",
    "    female_validation = validate_data_structure(str(female_file))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Load data if validation passes\n",
    "    dataframes = []\n",
    "    \n",
    "    if male_validation['file_readable'] and male_validation['required_columns_present']:\n",
    "        try:\n",
    "            male_df = pd.read_csv(male_file, low_memory=False)\n",
    "            male_df['gender'] = 'Male'\n",
    "            dataframes.append(male_df)\n",
    "            print(f\"✅ Loaded {len(male_df):,} male players\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading male players: {e}\")\n",
    "    \n",
    "    if female_validation['file_readable'] and female_validation['required_columns_present']:\n",
    "        try:\n",
    "            female_df = pd.read_csv(female_file, low_memory=False)\n",
    "            female_df['gender'] = 'Female'\n",
    "            dataframes.append(female_df)\n",
    "            print(f\"✅ Loaded {len(female_df):,} female players\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading female players: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"🎉 Total players loaded: {len(combined_df):,}\")\n",
    "        \n",
    "        # Basic data info\n",
    "        print(f\"\\n📊 Dataset Overview:\")\n",
    "        print(f\"   Shape: {combined_df.shape}\")\n",
    "        print(f\"   Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        print(f\"   Age range: {combined_df['age'].min()}-{combined_df['age'].max()}\")\n",
    "        print(f\"   Overall range: {combined_df['overall'].min()}-{combined_df['overall'].max()}\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"❌ No valid data files found\")\n",
    "        return None\n",
    "\n",
    "# Execute data validation and loading\n",
    "fifa_data = load_fifa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b4d7f",
   "metadata": {},
   "source": [
    "# 🚀 Section 4: Running Streamlit Applications\n",
    "\n",
    "## Dashboard Application Commands\n",
    "\n",
    "The FIFA Analytics Dashboard consists of three specialized applications, each serving different analytical purposes. Here are the exact commands to run each application:\n",
    "\n",
    "### Application Overview\n",
    "\n",
    "1. **Main Dashboard** (`app.py`): \n",
    "   - Overview statistics and distributions\n",
    "   - Individual player analysis\n",
    "   - Player comparisons\n",
    "   - Basic ML insights\n",
    "\n",
    "2. **Advanced Analytics** (`advanced_analytics.py`):\n",
    "   - Comprehensive player reports\n",
    "   - Market analysis and trends\n",
    "   - Advanced ML predictions\n",
    "   - League comparisons\n",
    "\n",
    "3. **Tactical Analysis** (`tactical_analysis.py`):\n",
    "   - Player clustering analysis\n",
    "   - Formation optimization\n",
    "   - Team building tools\n",
    "   - Strategic insights\n",
    "\n",
    "## Running Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "\n",
    "class StreamlitManager:\n",
    "    \"\"\"Manage multiple Streamlit applications\"\"\"\n",
    "    \n",
    "    def __init__(self, project_path: str = \".\"):\n",
    "        self.project_path = Path(project_path)\n",
    "        self.processes = {}\n",
    "        self.apps = {\n",
    "            \"main\": {\n",
    "                \"file\": \"app.py\",\n",
    "                \"port\": 8501,\n",
    "                \"name\": \"Main Dashboard\",\n",
    "                \"description\": \"Core analytics and player comparison\"\n",
    "            },\n",
    "            \"advanced\": {\n",
    "                \"file\": \"advanced_analytics.py\", \n",
    "                \"port\": 8502,\n",
    "                \"name\": \"Advanced Analytics\",\n",
    "                \"description\": \"ML predictions and market analysis\"\n",
    "            },\n",
    "            \"tactical\": {\n",
    "                \"file\": \"tactical_analysis.py\",\n",
    "                \"port\": 8503,\n",
    "                \"name\": \"Tactical Analysis\", \n",
    "                \"description\": \"Team building and formations\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def show_commands(self):\n",
    "        \"\"\"Display all available Streamlit commands\"\"\"\n",
    "        print(\"⚽ FIFA Dashboard - Streamlit Commands\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        print(\"\\n🎯 Individual Application Commands:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for app_key, app_info in self.apps.items():\n",
    "            print(f\"\\n📊 {app_info['name']}:\")\n",
    "            print(f\"   File: {app_info['file']}\")\n",
    "            print(f\"   Port: {app_info['port']}\")\n",
    "            print(f\"   Description: {app_info['description']}\")\n",
    "            print(f\"   Command: streamlit run {app_info['file']} --server.port {app_info['port']}\")\n",
    "            print(f\"   URL: http://localhost:{app_info['port']}\")\n",
    "        \n",
    "        print(f\"\\n🔧 Advanced Commands:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(\"🎨 Custom theme:\")\n",
    "        print(\"   streamlit run app.py --theme.primaryColor '#1f77b4'\")\n",
    "        \n",
    "        print(\"\\n🌐 Network access:\")\n",
    "        print(\"   streamlit run app.py --server.address 0.0.0.0\")\n",
    "        \n",
    "        print(\"\\n🐛 Debug mode:\")\n",
    "        print(\"   streamlit run app.py --logger.level debug\")\n",
    "        \n",
    "        print(\"\\n⚡ Performance:\")\n",
    "        print(\"   streamlit run app.py --server.maxUploadSize 1024\")\n",
    "        \n",
    "        print(\"\\n🛑 Stop all applications:\")\n",
    "        print(\"   Use Ctrl+C in each terminal or close terminal windows\")\n",
    "    \n",
    "    def check_file_exists(self, filename: str) -> bool:\n",
    "        \"\"\"Check if Streamlit app file exists\"\"\"\n",
    "        file_path = self.project_path / filename\n",
    "        exists = file_path.exists()\n",
    "        \n",
    "        if exists:\n",
    "            print(f\"✅ Found: {filename}\")\n",
    "        else:\n",
    "            print(f\"❌ Missing: {filename}\")\n",
    "            print(f\"   Expected path: {file_path}\")\n",
    "        \n",
    "        return exists\n",
    "    \n",
    "    def validate_apps(self):\n",
    "        \"\"\"Validate all application files exist\"\"\"\n",
    "        print(\"🔍 Validating Application Files\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        all_valid = True\n",
    "        for app_key, app_info in self.apps.items():\n",
    "            valid = self.check_file_exists(app_info['file'])\n",
    "            if not valid:\n",
    "                all_valid = False\n",
    "        \n",
    "        if all_valid:\n",
    "            print(\"\\n✅ All application files found!\")\n",
    "            print(\"🚀 Ready to launch dashboards!\")\n",
    "        else:\n",
    "            print(\"\\n❌ Some application files are missing.\")\n",
    "            print(\"📁 Please ensure you're in the correct directory.\")\n",
    "        \n",
    "        return all_valid\n",
    "    \n",
    "    def get_launch_instructions(self):\n",
    "        \"\"\"Get step-by-step launch instructions\"\"\"\n",
    "        instructions = [\n",
    "            \"🚀 Step-by-Step Launch Instructions\",\n",
    "            \"=\" * 40,\n",
    "            \"\",\n",
    "            \"1️⃣ Open three separate terminal/command prompt windows\",\n",
    "            \"\",\n",
    "            \"2️⃣ Navigate to project directory in each:\",\n",
    "            f'   cd \"{self.project_path}\"',\n",
    "            \"\",\n",
    "            \"3️⃣ Activate virtual environment in each:\",\n",
    "            \"   Windows: venv\\\\Scripts\\\\activate\",\n",
    "            \"   macOS/Linux: source venv/bin/activate\",\n",
    "            \"\",\n",
    "            \"4️⃣ Run applications in separate terminals:\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        for i, (app_key, app_info) in enumerate(self.apps.items(), 1):\n",
    "            instructions.extend([\n",
    "                f\"   Terminal {i} - {app_info['name']}:\",\n",
    "                f\"   streamlit run {app_info['file']} --server.port {app_info['port']}\",\n",
    "                \"\"\n",
    "            ])\n",
    "        \n",
    "        instructions.extend([\n",
    "            \"5️⃣ Access dashboards in browser:\",\n",
    "            \"\"\n",
    "        ])\n",
    "        \n",
    "        for app_key, app_info in self.apps.items():\n",
    "            instructions.append(f\"   {app_info['name']}: http://localhost:{app_info['port']}\")\n",
    "        \n",
    "        instructions.extend([\n",
    "            \"\",\n",
    "            \"💡 Pro Tips:\",\n",
    "            \"   • Use Ctrl+C to stop an application\",\n",
    "            \"   • Applications auto-reload on code changes\",\n",
    "            \"   • Check browser console for any errors\",\n",
    "            \"   • Use different browsers for multiple apps\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(instructions)\n",
    "\n",
    "# Initialize Streamlit Manager\n",
    "streamlit_manager = StreamlitManager()\n",
    "\n",
    "# Show all available commands\n",
    "streamlit_manager.show_commands()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Validate applications\n",
    "apps_valid = streamlit_manager.validate_apps()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Show launch instructions\n",
    "if apps_valid:\n",
    "    print(streamlit_manager.get_launch_instructions())\n",
    "else:\n",
    "    print(\"⚠️  Fix missing files before launching applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c37f46",
   "metadata": {},
   "source": [
    "# 🌐 Section 5: Port Management and Configuration\n",
    "\n",
    "## Understanding Port Configuration\n",
    "\n",
    "When running multiple Streamlit applications simultaneously, each needs its own port to avoid conflicts. The FIFA Dashboard uses three different ports:\n",
    "\n",
    "- **Main Dashboard**: Port 8501 (default)\n",
    "- **Advanced Analytics**: Port 8502\n",
    "- **Tactical Analysis**: Port 8503\n",
    "\n",
    "## Port Management Strategies\n",
    "\n",
    "### Default Port Usage\n",
    "Streamlit uses port 8501 by default. When this port is occupied, Streamlit automatically tries the next available port (8502, 8503, etc.).\n",
    "\n",
    "### Explicit Port Configuration\n",
    "For consistent access URLs, explicitly specify ports using the `--server.port` parameter.\n",
    "\n",
    "### Port Conflict Resolution\n",
    "If a port is already in use, you'll see an error message. Here's how to handle common scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05521035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import psutil\n",
    "from typing import List, Dict\n",
    "\n",
    "class PortManager:\n",
    "    \"\"\"Manage ports for multiple Streamlit applications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.default_ports = {\n",
    "            'main_dashboard': 8501,\n",
    "            'advanced_analytics': 8502, \n",
    "            'tactical_analysis': 8503\n",
    "        }\n",
    "        self.port_range = range(8501, 8520)  # Check ports 8501-8519\n",
    "    \n",
    "    def check_port_availability(self, port: int) -> bool:\n",
    "        \"\"\"Check if a port is available\"\"\"\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "                sock.settimeout(1)\n",
    "                result = sock.connect_ex(('localhost', port))\n",
    "                return result != 0  # Port is available if connection fails\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def find_process_on_port(self, port: int) -> Dict:\n",
    "        \"\"\"Find which process is using a specific port\"\"\"\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "            try:\n",
    "                for conn in proc.connections():\n",
    "                    if conn.laddr.port == port:\n",
    "                        return {\n",
    "                            'pid': proc.info['pid'],\n",
    "                            'name': proc.info['name'],\n",
    "                            'cmdline': ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else 'N/A'\n",
    "                        }\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                continue\n",
    "        return None\n",
    "    \n",
    "    def scan_ports(self) -> Dict:\n",
    "        \"\"\"Scan all relevant ports and return status\"\"\"\n",
    "        print(\"🔍 Port Availability Scan\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        port_status = {}\n",
    "        \n",
    "        for app_name, port in self.default_ports.items():\n",
    "            available = self.check_port_availability(port)\n",
    "            port_status[port] = {\n",
    "                'app': app_name,\n",
    "                'available': available,\n",
    "                'process': None if available else self.find_process_on_port(port)\n",
    "            }\n",
    "            \n",
    "            status_icon = \"✅\" if available else \"❌\"\n",
    "            print(f\"{status_icon} Port {port} ({app_name}): {'Available' if available else 'In Use'}\")\n",
    "            \n",
    "            if not available:\n",
    "                process_info = port_status[port]['process']\n",
    "                if process_info:\n",
    "                    print(f\"   🔧 Process: {process_info['name']} (PID: {process_info['pid']})\")\n",
    "                    if 'streamlit' in process_info['cmdline'].lower():\n",
    "                        print(f\"   📊 Streamlit app detected\")\n",
    "        \n",
    "        return port_status\n",
    "    \n",
    "    def find_available_ports(self, count: int = 3) -> List[int]:\n",
    "        \"\"\"Find available ports for applications\"\"\"\n",
    "        available_ports = []\n",
    "        \n",
    "        for port in self.port_range:\n",
    "            if self.check_port_availability(port):\n",
    "                available_ports.append(port)\n",
    "                if len(available_ports) >= count:\n",
    "                    break\n",
    "        \n",
    "        return available_ports\n",
    "    \n",
    "    def generate_port_commands(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate Streamlit commands with appropriate ports\"\"\"\n",
    "        port_status = self.scan_ports()\n",
    "        commands = {}\n",
    "        \n",
    "        print(f\"\\n🚀 Recommended Commands:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        # Try to use default ports first, fallback to available ones\n",
    "        available_ports = self.find_available_ports()\n",
    "        port_assignments = {}\n",
    "        \n",
    "        for i, (app_name, default_port) in enumerate(self.default_ports.items()):\n",
    "            if port_status[default_port]['available']:\n",
    "                assigned_port = default_port\n",
    "            else:\n",
    "                assigned_port = available_ports[i] if i < len(available_ports) else default_port + 10\n",
    "            \n",
    "            port_assignments[app_name] = assigned_port\n",
    "        \n",
    "        # Generate commands\n",
    "        app_files = {\n",
    "            'main_dashboard': 'app.py',\n",
    "            'advanced_analytics': 'advanced_analytics.py',\n",
    "            'tactical_analysis': 'tactical_analysis.py'\n",
    "        }\n",
    "        \n",
    "        for app_name, port in port_assignments.items():\n",
    "            file_name = app_files[app_name]\n",
    "            command = f\"streamlit run {file_name} --server.port {port}\"\n",
    "            commands[app_name] = {\n",
    "                'command': command,\n",
    "                'port': port,\n",
    "                'url': f\"http://localhost:{port}\"\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n📊 {app_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"   Command: {command}\")\n",
    "            print(f\"   URL: http://localhost:{port}\")\n",
    "        \n",
    "        return commands\n",
    "    \n",
    "    def create_batch_files(self, commands: Dict) -> None:\n",
    "        \"\"\"Create batch files for easy application launching\"\"\"\n",
    "        print(f\"\\n📁 Creating Launch Scripts:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Windows batch files\n",
    "        for app_name, cmd_info in commands.items():\n",
    "            batch_content = f\"\"\"@echo off\n",
    "echo Starting {app_name.replace('_', ' ').title()}...\n",
    "echo URL: {cmd_info['url']}\n",
    "echo Press Ctrl+C to stop the application\n",
    "echo.\n",
    "{cmd_info['command']}\n",
    "pause\"\"\"\n",
    "            \n",
    "            batch_file = f\"run_{app_name}.bat\"\n",
    "            try:\n",
    "                with open(batch_file, 'w') as f:\n",
    "                    f.write(batch_content)\n",
    "                print(f\"✅ Created: {batch_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to create {batch_file}: {e}\")\n",
    "        \n",
    "        # PowerShell script for all apps\n",
    "        ps_content = \"\"\"# FIFA Dashboard Launcher\n",
    "Write-Host \"🚀 FIFA Analytics Dashboard Launcher\" -ForegroundColor Green\n",
    "Write-Host \"=\" * 45 -ForegroundColor Green\n",
    "\n",
    "$apps = @{\"\"\"\n",
    "        \n",
    "        for app_name, cmd_info in commands.items():\n",
    "            ps_content += f\"\"\"\n",
    "    \"{app_name}\" = @{{\n",
    "        \"command\" = \"{cmd_info['command']}\"\n",
    "        \"url\" = \"{cmd_info['url']}\"\n",
    "    }}\"\"\"\n",
    "        \n",
    "        ps_content += \"\"\"\n",
    "}\n",
    "\n",
    "Write-Host \"Choose an application to launch:\" -ForegroundColor Yellow\n",
    "$i = 1\n",
    "foreach ($app in $apps.Keys) {\n",
    "    Write-Host \"$i. $($app.Replace('_', ' '))\" -ForegroundColor Cyan\n",
    "    $i++\n",
    "}\n",
    "\n",
    "$choice = Read-Host \"Enter your choice (1-3)\"\n",
    "$appNames = @($apps.Keys)\n",
    "$selectedApp = $appNames[$choice - 1]\n",
    "\n",
    "if ($selectedApp) {\n",
    "    $appInfo = $apps[$selectedApp]\n",
    "    Write-Host \"🚀 Launching $($selectedApp.Replace('_', ' '))...\" -ForegroundColor Green\n",
    "    Write-Host \"📍 URL: $($appInfo.url)\" -ForegroundColor Yellow\n",
    "    Write-Host \"⏹️  Press Ctrl+C to stop\" -ForegroundColor Red\n",
    "    Write-Host \"\"\n",
    "    \n",
    "    # Execute the command\n",
    "    Invoke-Expression $appInfo.command\n",
    "} else {\n",
    "    Write-Host \"❌ Invalid choice\" -ForegroundColor Red\n",
    "}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            with open(\"launch_dashboard.ps1\", 'w') as f:\n",
    "                f.write(ps_content)\n",
    "            print(\"✅ Created: launch_dashboard.ps1\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create PowerShell script: {e}\")\n",
    "\n",
    "# Initialize Port Manager\n",
    "port_manager = PortManager()\n",
    "\n",
    "# Scan current port status\n",
    "current_status = port_manager.scan_ports()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Generate optimized commands\n",
    "optimized_commands = port_manager.generate_port_commands()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Create convenience scripts\n",
    "port_manager.create_batch_files(optimized_commands)\n",
    "\n",
    "print(f\"\\n💡 Pro Tips:\")\n",
    "print(\"- Use the generated batch files for easy launching\")\n",
    "print(\"- Run each app in a separate terminal window\")\n",
    "print(\"- Keep terminals open while using the dashboards\")\n",
    "print(\"- Use Ctrl+C to gracefully stop applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a23f2d",
   "metadata": {},
   "source": [
    "# ⚡ Section 6: Performance Optimization Techniques\n",
    "\n",
    "## Understanding Performance Challenges\n",
    "\n",
    "The FIFA Analytics Dashboard handles large datasets (potentially 10M+ player records). Without proper optimization, this can lead to:\n",
    "\n",
    "- **Slow loading times**: Long waits for data processing\n",
    "- **Memory issues**: Application crashes or system slowdowns  \n",
    "- **Poor user experience**: Laggy interactions and timeouts\n",
    "- **Resource consumption**: High CPU and RAM usage\n",
    "\n",
    "## Optimization Strategies\n",
    "\n",
    "### 1. Data Sampling\n",
    "For exploration and development, use representative samples of your data rather than the full dataset.\n",
    "\n",
    "### 2. Lazy Loading\n",
    "Load data only when needed, rather than loading everything upfront.\n",
    "\n",
    "### 3. Efficient Data Types\n",
    "Use appropriate pandas data types to reduce memory usage.\n",
    "\n",
    "### 4. Pagination\n",
    "Display data in chunks rather than all at once.\n",
    "\n",
    "### 5. Caching\n",
    "Store processed results to avoid repeated calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"Tools for optimizing FIFA dashboard performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory_usage_history = []\n",
    "        \n",
    "    def get_memory_usage(self) -> float:\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def optimize_dataframe_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optimize DataFrame data types to reduce memory usage\"\"\"\n",
    "        print(\"🔧 Optimizing DataFrame Data Types\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"📊 Initial memory usage: {initial_memory:.1f} MB\")\n",
    "        \n",
    "        optimized_df = df.copy()\n",
    "        \n",
    "        # Optimize integer columns\n",
    "        int_columns = df.select_dtypes(include=['int64']).columns\n",
    "        for col in int_columns:\n",
    "            max_val = df[col].max()\n",
    "            min_val = df[col].min()\n",
    "            \n",
    "            if min_val >= 0:  # Unsigned integers\n",
    "                if max_val < 255:\n",
    "                    optimized_df[col] = df[col].astype('uint8')\n",
    "                elif max_val < 65535:\n",
    "                    optimized_df[col] = df[col].astype('uint16')\n",
    "                elif max_val < 4294967295:\n",
    "                    optimized_df[col] = df[col].astype('uint32')\n",
    "            else:  # Signed integers\n",
    "                if min_val > -128 and max_val < 127:\n",
    "                    optimized_df[col] = df[col].astype('int8')\n",
    "                elif min_val > -32768 and max_val < 32767:\n",
    "                    optimized_df[col] = df[col].astype('int16')\n",
    "                elif min_val > -2147483648 and max_val < 2147483647:\n",
    "                    optimized_df[col] = df[col].astype('int32')\n",
    "        \n",
    "        # Optimize float columns\n",
    "        float_columns = df.select_dtypes(include=['float64']).columns\n",
    "        for col in float_columns:\n",
    "            optimized_df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        # Convert categorical columns\n",
    "        object_columns = df.select_dtypes(include=['object']).columns\n",
    "        categorical_threshold = 0.5  # Convert to category if unique ratio < 50%\n",
    "        \n",
    "        for col in object_columns:\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if unique_ratio < categorical_threshold:\n",
    "                optimized_df[col] = df[col].astype('category')\\n                print(f\\\"📝 Converted {col} to category (unique ratio: {unique_ratio:.2%})\\\")\\n        \\n        final_memory = optimized_df.memory_usage(deep=True).sum() / 1024**2\\n        memory_reduction = (initial_memory - final_memory) / initial_memory * 100\\n        \\n        print(f\\\"📊 Final memory usage: {final_memory:.1f} MB\\\")\\n        print(f\\\"✅ Memory reduction: {memory_reduction:.1f}%\\\")\\n        \\n        return optimized_df\\n    \\n    def create_sample_dataset(self, df: pd.DataFrame, sample_size: int = 10000, \\n                            strategy: str = 'random') -> pd.DataFrame:\\n        \\\"\\\"\\\"Create a representative sample of the dataset\\\"\\\"\\\"\\n        print(f\\\"🎯 Creating Sample Dataset ({strategy})\\\")\\n        print(\\\"=\\\" * 35)\\n        \\n        if len(df) <= sample_size:\\n            print(f\\\"📊 Dataset already small enough: {len(df)} rows\\\")\\n            return df\\n        \\n        if strategy == 'random':\\n            sample_df = df.sample(n=sample_size, random_state=42)\\n            print(f\\\"🎲 Random sampling: {len(sample_df)} rows\\\")\\n            \\n        elif strategy == 'stratified':\\n            # Stratified sampling by overall rating\\n            if 'overall' in df.columns:\\n                df['overall_bin'] = pd.cut(df['overall'], bins=10, labels=False)\\n                sample_df = df.groupby('overall_bin').apply(\\n                    lambda x: x.sample(min(len(x), sample_size // 10), random_state=42)\\n                ).reset_index(drop=True)\\n                sample_df = sample_df.drop('overall_bin', axis=1)\\n                print(f\\\"📊 Stratified sampling: {len(sample_df)} rows\\\")\\n            else:\\n                sample_df = df.sample(n=sample_size, random_state=42)\\n                print(f\\\"🎲 Fallback to random sampling: {len(sample_df)} rows\\\")\\n                \\n        elif strategy == 'top_performers':\\n            # Sample top performers by overall rating\\n            if 'overall' in df.columns:\\n                sample_df = df.nlargest(sample_size, 'overall')\\n                print(f\\\"🏆 Top performers sampling: {len(sample_df)} rows\\\")\\n            else:\\n                sample_df = df.head(sample_size)\\n                print(f\\\"📊 Head sampling: {len(sample_df)} rows\\\")\\n        \\n        else:\\n            sample_df = df.head(sample_size)\\n            print(f\\\"📊 Head sampling: {len(sample_df)} rows\\\")\\n        \\n        # Show sample characteristics\\n        if 'overall' in sample_df.columns:\\n            print(f\\\"📈 Overall range: {sample_df['overall'].min()}-{sample_df['overall'].max()}\\\")\\n            print(f\\\"📊 Average overall: {sample_df['overall'].mean():.1f}\\\")\\n        \\n        return sample_df\\n    \\n    def implement_lazy_loading(self, file_path: str, chunk_size: int = 10000) -> None:\\n        \\\"\\\"\\\"Demonstrate lazy loading with pandas chunks\\\"\\\"\\\"\\n        print(f\\\"⚡ Lazy Loading Demo (chunk_size={chunk_size})\\\")\\n        print(\\\"=\\\" * 45)\\n        \\n        try:\\n            chunk_reader = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\\n            \\n            chunk_count = 0\\n            total_rows = 0\\n            memory_usage = []\\n            \\n            start_time = time.time()\\n            \\n            for chunk in chunk_reader:\\n                chunk_count += 1\\n                total_rows += len(chunk)\\n                current_memory = self.get_memory_usage()\\n                memory_usage.append(current_memory)\\n                \\n                # Process chunk (example: calculate statistics)\\n                if 'overall' in chunk.columns:\\n                    chunk_avg = chunk['overall'].mean()\\n                    print(f\\\"📊 Chunk {chunk_count}: {len(chunk)} rows, avg overall: {chunk_avg:.1f}\\\")\\n                \\n                # Memory management\\n                if chunk_count % 10 == 0:\\n                    gc.collect()  # Force garbage collection\\n                    print(f\\\"🧹 Memory cleanup at chunk {chunk_count}\\\")\\n                \\n                # Stop after a few chunks for demo\\n                if chunk_count >= 5:\\n                    print(f\\\"⏹️  Stopping demo after {chunk_count} chunks\\\")\\n                    break\\n            \\n            end_time = time.time()\\n            processing_time = end_time - start_time\\n            \\n            print(f\\\"\\\\n📊 Lazy Loading Results:\\\")\\n            print(f\\\"   Chunks processed: {chunk_count}\\\")\\n            print(f\\\"   Total rows: {total_rows:,}\\\")\\n            print(f\\\"   Processing time: {processing_time:.2f} seconds\\\")\\n            print(f\\\"   Average memory: {np.mean(memory_usage):.1f} MB\\\")\\n            print(f\\\"   Peak memory: {max(memory_usage):.1f} MB\\\")\\n            \\n        except Exception as e:\\n            print(f\\\"❌ Error during lazy loading: {e}\\\")\\n    \\n    def benchmark_operations(self, df: pd.DataFrame) -> Dict:\\n        \\\"\\\"\\\"Benchmark common operations on the dataset\\\"\\\"\\\"\\n        print(\\\"🏃‍♂️ Performance Benchmarking\\\")\\n        print(\\\"=\\\" * 30)\\n        \\n        benchmarks = {}\\n        \\n        operations = [\\n            ('Data Loading', lambda: df.copy()),\\n            ('Basic Statistics', lambda: df.describe()),\\n            ('Filtering', lambda: df[df['overall'] > 80] if 'overall' in df.columns else df.head()),\\n            ('Sorting', lambda: df.sort_values('overall') if 'overall' in df.columns else df),\\n            ('Grouping', lambda: df.groupby('player_positions')['overall'].mean() if all(col in df.columns for col in ['player_positions', 'overall']) else None)\\n        ]\\n        \\n        for operation_name, operation_func in operations:\\n            try:\\n                start_memory = self.get_memory_usage()\\n                start_time = time.time()\\n                \\n                result = operation_func()\\n                \\n                end_time = time.time()\\n                end_memory = self.get_memory_usage()\\n                \\n                execution_time = end_time - start_time\\n                memory_delta = end_memory - start_memory\\n                \\n                benchmarks[operation_name] = {\\n                    'time': execution_time,\\n                    'memory_delta': memory_delta,\\n                    'result_size': len(result) if hasattr(result, '__len__') else 'N/A'\\n                }\\n                \\n                print(f\\\"⏱️  {operation_name}: {execution_time:.3f}s, Memory: {memory_delta:+.1f}MB\\\")\\n                \\n            except Exception as e:\\n                print(f\\\"❌ {operation_name}: Error - {str(e)}\\\")\\n                benchmarks[operation_name] = {'error': str(e)}\\n        \\n        return benchmarks\\n    \\n    def get_optimization_recommendations(self, df: pd.DataFrame) -> List[str]:\\n        \\\"\\\"\\\"Generate performance optimization recommendations\\\"\\\"\\\"\\n        recommendations = []\\n        \\n        # Check dataset size\\n        dataset_size_mb = df.memory_usage(deep=True).sum() / 1024**2\\n        row_count = len(df)\\n        \\n        if dataset_size_mb > 100:\\n            recommendations.append(f\\\"🔧 Large dataset ({dataset_size_mb:.1f}MB) - Consider sampling for development\\\")\\n        \\n        if row_count > 100000:\\n            recommendations.append(f\\\"📊 Many rows ({row_count:,}) - Implement pagination in UI\\\")\\n        \\n        # Check data types\\n        memory_inefficient_cols = df.select_dtypes(include=['int64', 'float64']).columns\\n        if len(memory_inefficient_cols) > 5:\\n            recommendations.append(\\\"🔧 Optimize data types with downcast methods\\\")\\n        \\n        # Check for categorical data\\n        object_cols = df.select_dtypes(include=['object']).columns\\n        for col in object_cols:\\n            if df[col].nunique() / len(df) < 0.5:\\n                recommendations.append(f\\\"📝 Convert '{col}' to categorical data type\\\")\\n        \\n        # Check for missing values\\n        missing_data = df.isnull().sum().sum()\\n        if missing_data > len(df) * 0.1:\\n            recommendations.append(\\\"🧹 High missing data - implement efficient handling\\\")\\n        \\n        return recommendations\\n\\n# Example usage with mock data if FIFA data not available\\nprint(\\\"🚀 Performance Optimization Demo\\\")\\nprint(\\\"=\\\" * 35)\\n\\n# Create a mock dataset for demonstration\\nnp.random.seed(42)\\nmock_data = {\\n    'player_id': range(50000),\\n    'overall': np.random.randint(50, 100, 50000),\\n    'potential': np.random.randint(50, 100, 50000),\\n    'age': np.random.randint(16, 40, 50000),\\n    'value_eur': np.random.exponential(1000000, 50000),\\n    'player_positions': np.random.choice(['ST', 'CM', 'CB', 'GK', 'LW'], 50000),\\n    'league_name': np.random.choice(['Premier League', 'La Liga', 'Serie A', 'Bundesliga'], 50000)\\n}\\n\\nmock_df = pd.DataFrame(mock_data)\\nprint(f\\\"📊 Created mock dataset: {len(mock_df):,} rows\\\")\\n\\n# Initialize optimizer\\noptimizer = PerformanceOptimizer()\\n\\n# Run optimizations\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\noptimized_df = optimizer.optimize_dataframe_dtypes(mock_df)\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\nsample_df = optimizer.create_sample_dataset(optimized_df, sample_size=5000)\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\nbenchmark_results = optimizer.benchmark_operations(sample_df)\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\nrecommendations = optimizer.get_optimization_recommendations(mock_df)\\nprint(\\\"💡 Optimization Recommendations:\\\")\\nfor rec in recommendations:\\n    print(f\\\"   {rec}\\\")\\n\\nprint(\\\"\\\\n✅ Performance optimization demo completed!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303c19b",
   "metadata": {},
   "source": [
    "# 💾 Section 7: Data Caching Implementation\n",
    "\n",
    "## Understanding Streamlit Caching\n",
    "\n",
    "Streamlit's caching system is crucial for performance, especially with large datasets. The `@st.cache_data` decorator stores function results and serves cached copies when the same inputs are provided.\n",
    "\n",
    "## Benefits of Caching\n",
    "\n",
    "- **Faster Load Times**: Avoid reprocessing data on every interaction\n",
    "- **Better User Experience**: Immediate responses for cached operations\n",
    "- **Resource Efficiency**: Reduce CPU and memory usage\n",
    "- **Scalability**: Handle more users with the same resources\n",
    "\n",
    "## Caching Best Practices\n",
    "\n",
    "### 1. Cache Data Loading\n",
    "Always cache expensive data loading operations.\n",
    "\n",
    "### 2. Cache Preprocessing\n",
    "Store results of data cleaning and transformation.\n",
    "\n",
    "### 3. Cache Model Training\n",
    "Save trained machine learning models.\n",
    "\n",
    "### 4. Manage Cache Size\n",
    "Clear cache when data changes or memory is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This demonstrates caching patterns used in the FIFA Dashboard\\n# In an actual Streamlit app, these would be used with @st.cache_data decorator\\n\\nimport hashlib\\nimport pickle\\nimport time\\nfrom functools import wraps\\nfrom typing import Any, Callable, Dict, Optional\\nimport os\\n\\nclass CacheManager:\\n    \\\"\\\"\\\"Simulate Streamlit's caching behavior for demonstration\\\"\\\"\\\"\\n    \\n    def __init__(self, cache_dir: str = \\\".cache\\\"):\\n        self.cache_dir = cache_dir\\n        self.cache_stats = {'hits': 0, 'misses': 0}\\n        os.makedirs(cache_dir, exist_ok=True)\\n    \\n    def _get_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:\\n        \\\"\\\"\\\"Generate a unique cache key for function call\\\"\\\"\\\"\\n        key_data = f\\\"{func_name}_{str(args)}_{str(sorted(kwargs.items()))}\\\"\\n        return hashlib.md5(key_data.encode()).hexdigest()\\n    \\n    def _get_cache_path(self, cache_key: str) -> str:\\n        \\\"\\\"\\\"Get file path for cached result\\\"\\\"\\\"\\n        return os.path.join(self.cache_dir, f\\\"{cache_key}.cache\\\")\\n    \\n    def cache_data(self, ttl: Optional[int] = None, show_spinner: bool = True):\\n        \\\"\\\"\\\"Decorator to cache function results (simulates @st.cache_data)\\\"\\\"\\\"\\n        def decorator(func: Callable) -> Callable:\\n            @wraps(func)\\n            def wrapper(*args, **kwargs):\\n                # Generate cache key\\n                cache_key = self._get_cache_key(func.__name__, args, kwargs)\\n                cache_path = self._get_cache_path(cache_key)\\n                \\n                # Check if cached result exists and is valid\\n                if os.path.exists(cache_path):\\n                    if ttl is None or (time.time() - os.path.getmtime(cache_path)) < ttl:\\n                        try:\\n                            with open(cache_path, 'rb') as f:\\n                                result = pickle.load(f)\\n                            self.cache_stats['hits'] += 1\\n                            print(f\\\"✅ Cache hit for {func.__name__}\\\")\\n                            return result\\n                        except Exception as e:\\n                            print(f\\\"⚠️  Cache read error: {e}\\\")\\n                \\n                # Cache miss - execute function\\n                self.cache_stats['misses'] += 1\\n                print(f\\\"🔄 Cache miss for {func.__name__} - executing...\\\")\\n                \\n                if show_spinner:\\n                    print(f\\\"⏳ Running {func.__name__}...\\\")\\n                \\n                start_time = time.time()\\n                result = func(*args, **kwargs)\\n                execution_time = time.time() - start_time\\n                \\n                print(f\\\"⚡ {func.__name__} completed in {execution_time:.2f}s\\\")\\n                \\n                # Save result to cache\\n                try:\\n                    with open(cache_path, 'wb') as f:\\n                        pickle.dump(result, f)\\n                    print(f\\\"💾 Cached result for {func.__name__}\\\")\\n                except Exception as e:\\n                    print(f\\\"⚠️  Cache write error: {e}\\\")\\n                \\n                return result\\n            \\n            return wrapper\\n        return decorator\\n    \\n    def clear_cache(self, pattern: Optional[str] = None):\\n        \\\"\\\"\\\"Clear cached results\\\"\\\"\\\"\\n        cleared_count = 0\\n        \\n        for filename in os.listdir(self.cache_dir):\\n            if filename.endswith('.cache'):\\n                if pattern is None or pattern in filename:\\n                    file_path = os.path.join(self.cache_dir, filename)\\n                    try:\\n                        os.remove(file_path)\\n                        cleared_count += 1\\n                    except Exception as e:\\n                        print(f\\\"❌ Error removing {filename}: {e}\\\")\\n        \\n        print(f\\\"🧹 Cleared {cleared_count} cache files\\\")\\n        self.cache_stats = {'hits': 0, 'misses': 0}\\n    \\n    def get_cache_stats(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get cache performance statistics\\\"\\\"\\\"\\n        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']\\n        hit_rate = (self.cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0\\n        \\n        cache_files = [f for f in os.listdir(self.cache_dir) if f.endswith('.cache')]\\n        total_cache_size = sum(os.path.getsize(os.path.join(self.cache_dir, f)) for f in cache_files)\\n        \\n        return {\\n            'hits': self.cache_stats['hits'],\\n            'misses': self.cache_stats['misses'],\\n            'hit_rate': hit_rate,\\n            'cache_files': len(cache_files),\\n            'total_size_mb': total_cache_size / 1024**2\\n        }\\n\\n# Initialize cache manager\\ncache_manager = CacheManager()\\n\\n# Example cached functions for FIFA Dashboard\\n\\n@cache_manager.cache_data(ttl=3600)  # Cache for 1 hour\\ndef load_fifa_data_cached(file_path: str) -> pd.DataFrame:\\n    \\\"\\\"\\\"Cached data loading function\\\"\\\"\\\"\\n    print(f\\\"📁 Loading data from {file_path}...\\\")\\n    time.sleep(2)  # Simulate loading time\\n    \\n    # In real implementation, this would load actual FIFA data\\n    np.random.seed(42)\\n    data = {\\n        'player_id': range(10000),\\n        'long_name': [f'Player_{i}' for i in range(10000)],\\n        'overall': np.random.randint(50, 100, 10000),\\n        'potential': np.random.randint(50, 100, 10000),\\n        'value_eur': np.random.exponential(1000000, 10000)\\n    }\\n    \\n    return pd.DataFrame(data)\\n\\n@cache_manager.cache_data(ttl=1800)  # Cache for 30 minutes\\ndef calculate_player_statistics(df: pd.DataFrame) -> Dict[str, float]:\\n    \\\"\\\"\\\"Cached statistics calculation\\\"\\\"\\\"\\n    print(\\\"📊 Calculating player statistics...\\\")\\n    time.sleep(1)  # Simulate processing time\\n    \\n    stats = {\\n        'total_players': len(df),\\n        'avg_overall': df['overall'].mean(),\\n        'avg_potential': df['potential'].mean(),\\n        'avg_value': df['value_eur'].mean(),\\n        'max_overall': df['overall'].max(),\\n        'min_overall': df['overall'].min()\\n    }\\n    \\n    return stats\\n\\n@cache_manager.cache_data(ttl=7200)  # Cache for 2 hours\\ndef train_value_prediction_model(df: pd.DataFrame) -> Dict[str, Any]:\\n    \\\"\\\"\\\"Cached ML model training\\\"\\\"\\\"\\n    print(\\\"🤖 Training value prediction model...\\\")\\n    time.sleep(3)  # Simulate training time\\n    \\n    # In real implementation, this would train an actual ML model\\n    model_info = {\\n        'model_type': 'RandomForestRegressor',\\n        'features': ['overall', 'potential'],\\n        'accuracy': 0.85,\\n        'training_samples': len(df),\\n        'trained_at': time.time()\\n    }\\n    \\n    return model_info\\n\\n@cache_manager.cache_data(ttl=600)  # Cache for 10 minutes\\ndef filter_players_by_criteria(df: pd.DataFrame, min_overall: int = 80, \\n                              max_age: int = 30) -> pd.DataFrame:\\n    \\\"\\\"\\\"Cached filtering operation\\\"\\\"\\\"\\n    print(f\\\"🔍 Filtering players (overall >= {min_overall}, age <= {max_age})...\\\")\\n    time.sleep(0.5)  # Simulate processing time\\n    \\n    # Add age column for filtering demo\\n    df['age'] = np.random.randint(16, 40, len(df))\\n    \\n    filtered_df = df[\\n        (df['overall'] >= min_overall) & \\n        (df['age'] <= max_age)\\n    ]\\n    \\n    return filtered_df\\n\\n# Demonstrate caching behavior\\nprint(\\\"🧪 Caching Demonstration\\\")\\nprint(\\\"=\\\" * 25)\\n\\n# First calls - cache misses\\nprint(\\\"\\\\n🔄 First execution (cache misses):\\\")\\nprint(\\\"-\\\" * 35)\\n\\ndata_df = load_fifa_data_cached(\\\"data/players.csv\\\")\\nstats = calculate_player_statistics(data_df)\\nmodel = train_value_prediction_model(data_df)\\nfiltered = filter_players_by_criteria(data_df, min_overall=85)\\n\\nprint(f\\\"\\\\n📊 Statistics: {stats}\\\")\\nprint(f\\\"🤖 Model info: {model['model_type']}, Accuracy: {model['accuracy']}\\\")\\nprint(f\\\"🔍 Filtered players: {len(filtered)}\\\")\\n\\n# Second calls - cache hits\\nprint(\\\"\\\\n⚡ Second execution (cache hits):\\\")\\nprint(\\\"-\\\" * 35)\\n\\ndata_df = load_fifa_data_cached(\\\"data/players.csv\\\")\\nstats = calculate_player_statistics(data_df)\\nmodel = train_value_prediction_model(data_df)\\nfiltered = filter_players_by_criteria(data_df, min_overall=85)\\n\\n# Show cache statistics\\nprint(\\\"\\\\n📈 Cache Performance:\\\")\\nprint(\\\"-\\\" * 20)\\ncache_stats = cache_manager.get_cache_stats()\\nfor key, value in cache_stats.items():\\n    if key == 'hit_rate':\\n        print(f\\\"   {key}: {value:.1f}%\\\")\\n    elif key == 'total_size_mb':\\n        print(f\\\"   {key}: {value:.2f} MB\\\")\\n    else:\\n        print(f\\\"   {key}: {value}\\\")\\n\\n# Cache management examples\\nprint(\\\"\\\\n🧹 Cache Management:\\\")\\nprint(\\\"-\\\" * 20)\\n\\n# Show cache optimization recommendations\\ndef get_cache_recommendations(stats: Dict) -> List[str]:\\n    recommendations = []\\n    \\n    if stats['hit_rate'] < 50:\\n        recommendations.append(\\\"🔧 Low hit rate - consider longer TTL values\\\")\\n    \\n    if stats['total_size_mb'] > 100:\\n        recommendations.append(\\\"💾 Large cache size - consider periodic cleanup\\\")\\n    \\n    if stats['cache_files'] > 50:\\n        recommendations.append(\\\"🗂️  Many cache files - implement cache rotation\\\")\\n    \\n    return recommendations\\n\\nrecommendations = get_cache_recommendations(cache_stats)\\nif recommendations:\\n    print(\\\"💡 Cache Optimization Recommendations:\\\")\\n    for rec in recommendations:\\n        print(f\\\"   {rec}\\\")\\nelse:\\n    print(\\\"✅ Cache performance is optimal\\\")\\n\\nprint(\\\"\\\\n🎯 Caching Best Practices for FIFA Dashboard:\\\")\\nprint(\\\"   1. Cache data loading operations (@st.cache_data)\\\")\\nprint(\\\"   2. Use appropriate TTL values (1-24 hours for data)\\\")\\nprint(\\\"   3. Cache ML model training (longer TTL)\\\")\\nprint(\\\"   4. Cache expensive calculations and aggregations\\\")\\nprint(\\\"   5. Clear cache when underlying data changes\\\")\\nprint(\\\"   6. Monitor cache hit rates and sizes\\\")\\nprint(\\\"   7. Use hash_funcs for complex objects if needed\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f75e5",
   "metadata": {},
   "source": [
    "# 🛠️ Section 8: Error Handling and Troubleshooting\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### 1. Import Errors\n",
    "```python\n",
    "# Issue: ModuleNotFoundError\n",
    "# Solution: Install missing packages\n",
    "pip install streamlit pandas plotly scikit-learn\n",
    "\n",
    "# Issue: Version conflicts\n",
    "# Solution: Use virtual environment and specific versions\n",
    "pip install streamlit==1.28.1\n",
    "```\n",
    "\n",
    "### 2. Data Loading Problems\n",
    "```python\n",
    "# Issue: File not found\n",
    "# Solution: Check file paths and working directory\n",
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in data/:\", os.listdir(\"data/\"))\n",
    "\n",
    "# Issue: Encoding problems\n",
    "# Solution: Specify encoding when reading CSV\n",
    "df = pd.read_csv(\"data/players.csv\", encoding='utf-8')\n",
    "```\n",
    "\n",
    "### 3. Memory Issues\n",
    "```python\n",
    "# Issue: Out of memory\n",
    "# Solutions:\n",
    "# - Use data sampling\n",
    "df_sample = df.sample(n=10000)\n",
    "\n",
    "# - Optimize data types\n",
    "df = df.astype({'player_id': 'uint32', 'overall': 'uint8'})\n",
    "\n",
    "# - Use chunked processing\n",
    "for chunk in pd.read_csv(\"large_file.csv\", chunksize=1000):\n",
    "    process_chunk(chunk)\n",
    "```\n",
    "\n",
    "### 4. Port Conflicts\n",
    "```bash\n",
    "# Issue: Address already in use\n",
    "# Solutions:\n",
    "streamlit run app.py --server.port 8502\n",
    "# OR kill existing processes\n",
    "netstat -ano | findstr :8501  # Windows\n",
    "lsof -ti:8501 | xargs kill   # macOS/Linux\n",
    "```\n",
    "\n",
    "# ⚡ Section 9: Application Testing and Validation\n",
    "\n",
    "## Testing Checklist\n",
    "\n",
    "### Data Validation Tests\n",
    "```python\n",
    "def test_data_integrity(df):\n",
    "    \"\"\"Test data quality and integrity\"\"\"\n",
    "    tests = []\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['player_id', 'long_name', 'overall']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    tests.append((\"Required columns\", len(missing_cols) == 0, missing_cols))\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df['player_id'].duplicated().sum()\n",
    "    tests.append((\"No duplicates\", duplicates == 0, f\"{duplicates} duplicates\"))\n",
    "    \n",
    "    # Check value ranges\n",
    "    valid_overall = df['overall'].between(0, 100).all()\n",
    "    tests.append((\"Overall range 0-100\", valid_overall, \"Invalid overall values\"))\n",
    "    \n",
    "    return tests\n",
    "```\n",
    "\n",
    "### Performance Tests\n",
    "```python\n",
    "def test_performance_benchmarks():\n",
    "    \"\"\"Test application performance\"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Test data loading speed\n",
    "    start = time.time()\n",
    "    df = load_fifa_data()\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    print(f\"Data loading: {load_time:.2f}s {'✅' if load_time < 10 else '❌'}\")\n",
    "    \n",
    "    # Test filtering speed\n",
    "    start = time.time()\n",
    "    filtered = df[df['overall'] > 80]\n",
    "    filter_time = time.time() - start\n",
    "    \n",
    "    print(f\"Filtering: {filter_time:.2f}s {'✅' if filter_time < 1 else '❌'}\")\n",
    "```\n",
    "\n",
    "# 🚀 Section 10: Deployment Configuration\n",
    "\n",
    "## Production Deployment Setup\n",
    "\n",
    "### 1. Environment Variables\n",
    "```python\n",
    "# config.py\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "class Config:\n",
    "    # Data settings\n",
    "    DATA_PATH = os.getenv('FIFA_DATA_PATH', 'data/')\n",
    "    SAMPLE_SIZE = int(os.getenv('FIFA_SAMPLE_SIZE', '10000'))\n",
    "    \n",
    "    # Performance settings\n",
    "    CACHE_TTL = int(os.getenv('CACHE_TTL', '3600'))\n",
    "    MAX_WORKERS = int(os.getenv('MAX_WORKERS', '4'))\n",
    "    \n",
    "    # Streamlit settings\n",
    "    SERVER_PORT = int(os.getenv('PORT', '8501'))\n",
    "    SERVER_ADDRESS = os.getenv('SERVER_ADDRESS', '0.0.0.0')\n",
    "    \n",
    "    @classmethod\n",
    "    def get_streamlit_config(cls) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'server.port': cls.SERVER_PORT,\n",
    "            'server.address': cls.SERVER_ADDRESS,\n",
    "            'server.enableCORS': False,\n",
    "            'server.enableXsrfProtection': False\n",
    "        }\n",
    "```\n",
    "\n",
    "### 2. Docker Configuration\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.12-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8501\n",
    "\n",
    "HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n",
    "\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.address\", \"0.0.0.0\"]\n",
    "```\n",
    "\n",
    "### 3. Cloud Deployment Commands\n",
    "\n",
    "#### Streamlit Cloud\n",
    "```bash\n",
    "# 1. Push to GitHub\n",
    "git add .\n",
    "git commit -m \"Deploy FIFA Dashboard\"\n",
    "git push origin main\n",
    "\n",
    "# 2. Connect repository in Streamlit Cloud\n",
    "# 3. Set environment variables in dashboard\n",
    "```\n",
    "\n",
    "#### Heroku Deployment\n",
    "```bash\n",
    "# 1. Create Procfile\n",
    "echo \"web: streamlit run app.py --server.port=$PORT --server.address=0.0.0.0\" > Procfile\n",
    "\n",
    "# 2. Deploy to Heroku\n",
    "heroku create fifa-dashboard\n",
    "git push heroku main\n",
    "heroku open\n",
    "```\n",
    "\n",
    "#### Docker Deployment\n",
    "```bash\n",
    "# 1. Build image\n",
    "docker build -t fifa-dashboard .\n",
    "\n",
    "# 2. Run container\n",
    "docker run -p 8501:8501 -e FIFA_SAMPLE_SIZE=5000 fifa-dashboard\n",
    "\n",
    "# 3. Deploy to cloud\n",
    "docker tag fifa-dashboard your-registry/fifa-dashboard\n",
    "docker push your-registry/fifa-dashboard\n",
    "```\n",
    "\n",
    "## 🎉 Conclusion and Next Steps\n",
    "\n",
    "### Summary of Achievements\n",
    "✅ **Complete Setup Guide**: Environment, dependencies, and data validation  \n",
    "✅ **Command Reference**: All Streamlit application commands  \n",
    "✅ **Performance Optimization**: Caching, sampling, and memory management  \n",
    "✅ **Error Handling**: Comprehensive troubleshooting guide  \n",
    "✅ **Production Ready**: Deployment configurations and best practices  \n",
    "\n",
    "### Next Steps for Enhancement\n",
    "1. **Real-time Data Integration**: Connect to live football APIs\n",
    "2. **Advanced ML Models**: Implement neural networks and ensemble methods\n",
    "3. **User Authentication**: Add login and personalization features\n",
    "4. **Mobile Optimization**: Responsive design improvements\n",
    "5. **Multi-language Support**: Internationalization features\n",
    "\n",
    "### Resources for Continued Learning\n",
    "- **Streamlit Documentation**: https://docs.streamlit.io/\n",
    "- **FIFA Data Sources**: https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset\n",
    "- **Performance Optimization**: https://docs.streamlit.io/library/advanced-features/caching\n",
    "- **Deployment Guides**: https://docs.streamlit.io/streamlit-community-cloud\n",
    "\n",
    "---\n",
    "\n",
    "**🚀 Your FIFA Analytics Dashboard is now ready for professional deployment!**\n",
    "\n",
    "*Happy analyzing! ⚽📊*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
